%! Author = jgilmer19
%! Date = 1/28/20

% Preamble
\documentclass[12pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{listings}

% Document
\begin{document}
    \noindent John Gilmer \\
    \noindent SDS 383 D - Statistical Modeling 2 \\
    \noindent Homework \\

    \noindent\textbf{Exercise 1.1}

    \(X_1, \dots X_n \sim Bernoulli(p), \quad h(p) \sim Beta(\alpha, \beta)\)

    \begin{align*}
        p | X_1, \dots X_N \propto& f(x_1, \dots x_n |p) \cdot h(p) \\
        \propto&  p^{\sum x_i} (1 - p)^{n- \sum x_i} \cdot p^{\alpha - 1} (1 - p) ^{\beta - 1} \\
        \propto&  p^{\sum x_i + \alpha - 1} (1 - p)^{\beta + n - \sum x_i - 1}
    \end{align*}

    \begin{align*}
      p | X_1, \dots X_N \sim Beta(\sum x_i + \alpha, \beta + n - \sum x_i)
    \end{align*}
    \\

    \noindent\textbf{Exercise 1.2} \\ \\
    \(X_1, \dots X_N \stackrel{iid}{\sim} Cat(p), \quad p = (p_1, \dots p_K), \quad
       f(x | p) = \prod_{i=1}^{n} \prod_{k=1}^{K} p_k ^{I(x_i = k)}\) \\
    \( p \sim Dirichlet(\alpha_1, \dots \alpha_K)\) \\
    \begin{equation*}
    \pi(p) = \frac{\Gamma (\sum_k \alpha_k)}{\prod_K \Gamma(\alpha_k)} \prod_{k=1}^{K}
        p^{\alpha_k - 1} \mdot I(\sum_{k=1}^{K-1} p_k \leq 1, p_k \geq 0)
    \end{equation*}

    \begin{align*}
        p | X_{1:n} \propto& f(x_{1:n} | p) \cdot \pi(p) \\
        \propto& \prod_{i=1} ^{n} \prod_{k=1}^K p_k ^{I(x_i = k)} \cdot \prod_{k=1}^{K} p_k^{\alpha_k -1} \\
        \propto & \prod_{k=1}^K p_K ^{n_k + \alpha_k -1}, \quad \text{where }n_k = \sum_{i=1}^{N} I(x_i = k) \\
        p | X_{1:n} \sim& Dir(n_1 + \alpha_1, \dots n_k + \alpha_k)
    \end{align*}

    \noindent \textbf{Exercise 1.3} \\ \\

    \(X \sim Gam(\alpha, b)\) and \(Y \sim Gam(\beta, b)\) and X is independent of Y. \\

    \textbf{1.3.a}

    Let \(W = X + Y\) and \(Z = X / (X + Y)\). Show that W and Z are independent with
    \(W \sim Gam(\alpha + \beta, b)\) and \(Z \sim Beta (\alpha, \beta)\).

    This is a bivariate random variable transformation:

    \begin{align*}
        & \text{Change of variables equation: } f_{w,z} = f_{x,y}(h_1 (w, z), h_2(w, z)) |J| \\
        & g_1(X, Y) = W = X + Y \\
        & g_2(X, Y) = Z = X / (X + Y) \\
        & h_1(Z, W) = X = ZW \\
        & h_2(Z, W) = Y = W (1 - Z) \\
        &|J| = abs \big( \begin{vmatrix} \frac{\partial X}{\partial W} &
          \frac{\partial X}{\partial Z} \\
          \frac{\partial Y}{\partial W} & \frac{\partial Y}{\partial Z}
          \end{vmatrix} \big) =
          \begin{vmatrix} Z & W \\ 1 - Z & -W \end{vmatrix}
          = abs(-w) = w \\
        &f_{w,z} = f_x(zw) \mdot f_y(w-zw) w \\
        &f_{w,z} = \frac{b^{\alpha} (zw)^{\alpha - 1} e^{-bzw} \cdot b^{\beta}
                       (w(1- z))^{\beta - 1} e^{-bw(1-z)} w}{\Gamma(\alpha) \Gamma(\beta)}
           \cdot I(zw \geq 0) \cdot I(w - zw \geq 0) \\
        &f_{w,z} = \frac{w^{\alpha + \beta - 1} b^{\alpha + \beta} e^{-bw}
              \cdot z^{\alpha - 1} (1 - z)^{\beta -1}} {\Gamma(\alpha) \Gamma(\beta)}
              \cdot I(0 \leq z \leq 1) \cdot I(w \geq 0) \\
        & f_{w,z} = \frac{b^{\alpha + \beta} w^{\alpha + \beta -1} e^{-bw}
                   I( w \geq 0)}{\Gamma(\alpha + \beta)} \cdot
                   \frac{\Gamma(\alpha + \beta) z^{\alpha - 1} (1-z)^{\beta -1}
                   I( 0 \leq z \leq 1)}{\Gamma(\alpha) \Gamma(\beta)}
    \end{align*}

    So W and Z are independent by factorization theorem and \(W \sim Gam(\alpha + \beta, b)\) and
    \(Z \sim Beta(\alpha, \beta)\) \\

    \textbf{1.3.b} \\
    Explain how to use a random gamma sampler to sample from a \(Beta(\alpha, \beta)\) random variable
    using R:

    We can use the R function \textbf(rgamma(n, shape, rate)) to sample two gamma random variables
    with shape parameters \(\alpha = 10, \beta = 2\) and rate parameters equal \(b = 5, b= 5\) and then
    use the transformation from the previous step to get a sample from a \(Beta(\alpha = 10, \beta =2)\)
    distribution.

    \begin{lstlisting}[language=R]
      x <- rgamma(1, 10, 5)
      y <- rgamma(1, 2, 5)
      z = x / (x + y)
    \end{lstlisting} \\

    \noindent \textbf{Exercise 1.4} \\
    Suppose \(X_1 \dots X_n \stackrel{iid}{\sim} \text{Normal}(\theta, \sigma_0 ^2)\) where \(\sigma_0 ^2\)
    is known. Suppose \(\theta \sim \text{Normal}(m, v)\), find posterior of \(\theta | X_{1:N}\)

    \begin{align*}
      \theta | X_{1:N} \propto& \text{ exp}\big\{ \frac{-1}{2\sigma_0 ^2} \sum_{i=1}^{N} (x_i - \theta)^2 \big \}
         \cdot \text{ exp}\big\{ \frac{-1}{2v^2}(\theta - m)^2 \big \} \\
        \propto& \text{ exp} \big\{ \frac{-1}{2 \sigma_0 ^2} \big(\sum_{i=1}^N (x_i ^2 + \theta ^2 - 2 \theta \x_i)\big)
           + \frac{-1}{2 v^2} \big( \theta^2 + m^2 - 2 \theta m\big) \big \} \\
        \propto& \text{ exp} \big \{ \frac{-\theta ^2}{2} \cdot (\frac{1}{v^2} + \frac{n}{\sigma_0 ^2})
           + \theta (\frac{m}{v^2} + \frac{\sum x_i}{\sigma_0 ^2}) - (\frac{m^2}{2v^2}
           + \frac{\sum x_i^2}{2 \sigma_0 ^2}) \big \} \\
        \propto& \text{ exp} \big \{ \frac{-\theta ^2}{2} \cdot (\frac{1}{v^2} + \frac{n}{\sigma_0 ^2})
           + \theta (\frac{m}{v^2} + \frac{\sum x_i}{\sigma_0 ^2}) \big \} \\
        \propto& \text { exp} \big \{ \frac{-1}{2} \big(
               \theta ^2  (\frac{1}{v^2} + \frac{n}{\sigma_0 ^2})
               -  2 \theta (\frac{m}{v^2} + \frac{\sum x_i}{\sigma_0 ^2}) \big)
    \end{align*}

    \hline

    Completing the square:
    \begin{align*}
        &a =  \frac{1}{v^2} + \frac{n}{\sigma_0 ^2} \\
        &b =  \frac{m}{v^2} + \frac{\sum x_i}{\sigma_0 ^2} \\
        &a \theta ^2 - 2 \theta b = a ( \theta ^2 - \frac{2 \theta b}{a}) =
           a ( (\theta - \frac{b}{a})^2 - \frac{b^2}{a^2})
    \end{align*}
    \hline

    \begin{align*}
    \theta | X_{1:N} \propto& \text{ exp} \big \{ \frac{-1}{2} a (\theta - \frac{b}{a})^2 \big \} \\
        \propto&  \text{ exp} \big \{ \frac{-1}{2} \frac{(\theta - \frac{b}{a})}{\frac{1}{\sqrt{a}}}^2 \big \} \\
    \theta | X_{1:N} \sim& \text{Normal}(m = \frac{b}{a}, v = a^{-1}) \\
    &m = \frac{b}{a}  = \frac{\frac{m}{v^2} + \frac{\sum x_i}{\sigma_0 ^2}} {\frac{1}{v^2} + \frac{n}{\sigma_0 ^2} },
        \quad v = (\frac{1}{v^2} + \frac{n}{\sigma_0 ^2})^{-1}
    \end{align*}




\end{document}